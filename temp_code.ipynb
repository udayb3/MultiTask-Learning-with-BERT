{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, time, random, numpy as np, pandas as pd\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed= 10002):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets class to create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    return ' '.join(s.lower().replace('.', ' .').replace('?', ' ?').replace(',', ' ,').replace('\\'', ' \\'').split())\n",
    "\n",
    "\n",
    "class SentenceClassificationDataset(Dataset):\n",
    "    \"\"\"Inheriting the dataset class for the sentence classification task\"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset; self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def pad_data(self, data):\n",
    "        '''This function pads the data to the max length of the batch'''\n",
    "        sents = [x[0] for x in data]\n",
    "        labels = [x[1]-1 for x in data] # subtracting to accomodate for 0-indexed classes\n",
    "        sent_ids = [x[2] for x in data]\n",
    "\n",
    "        encoding = self.tokenizer(sents, return_tensors='pt', padding=True, truncation=True)\n",
    "        token_ids = torch.LongTensor(encoding['input_ids']);    attention_mask = torch.LongTensor(encoding['attention_mask']);  labels = torch.LongTensor(labels)\n",
    "\n",
    "        return token_ids, attention_mask, labels, sents, sent_ids\n",
    "\n",
    "    def collate_fn(self, all_data):\n",
    "        token_ids, attention_mask, labels, sents, sent_ids= self.pad_data(all_data)\n",
    "\n",
    "        batched_data = { 'token_ids': token_ids, 'attention_mask': attention_mask, 'labels': labels, 'sents': sents, 'sent_ids': sent_ids }\n",
    "        return batched_data\n",
    "\n",
    "class SentencePairDataset(Dataset):\n",
    "    def __init__(self, dataset, isRegression =False):\n",
    "        self.dataset = dataset\n",
    "        self.isRegression = isRegression\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def pad_data(self, data):\n",
    "        sent1 = [x[0] for x in data]\n",
    "        sent2 = [x[1] for x in data]\n",
    "        labels = [x[2] for x in data]\n",
    "        sent_ids = [x[3] for x in data]\n",
    "\n",
    "        encoding1 = self.tokenizer(sent1, return_tensors='pt', padding=True, truncation=True)\n",
    "        encoding2 = self.tokenizer(sent2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        token_ids = torch.LongTensor(encoding1['input_ids'])\n",
    "        attention_mask = torch.LongTensor(encoding1['attention_mask'])\n",
    "        token_type_ids = torch.LongTensor(encoding1['token_type_ids'])\n",
    "\n",
    "        token_ids2 = torch.LongTensor(encoding2['input_ids'])\n",
    "        attention_mask2 = torch.LongTensor(encoding2['attention_mask'])\n",
    "        token_type_ids2 = torch.LongTensor(encoding2['token_type_ids'])\n",
    "        if self.isRegression:\n",
    "            labels = torch.FloatTensor(labels)\n",
    "        else:\n",
    "            labels = torch.LongTensor(labels)\n",
    "            \n",
    "\n",
    "        return (token_ids, token_type_ids, attention_mask,\n",
    "                token_ids2, token_type_ids2, attention_mask2,\n",
    "                labels,sent_ids)\n",
    "\n",
    "    def collate_fn(self, all_data):\n",
    "        (token_ids, token_type_ids, attention_mask, token_ids2, token_type_ids2, attention_mask2, labels, sent_ids) = self.pad_data(all_data)\n",
    "\n",
    "        batched_data = { 'token_ids_1': token_ids, 'token_type_ids_1': token_type_ids, 'attention_mask_1': attention_mask, 'token_ids_2': token_ids2, 'token_type_ids_2': token_type_ids2, 'attention_mask_2': attention_mask2, 'labels': labels, 'sent_ids': sent_ids }\n",
    "        return batched_data\n",
    "\n",
    "def load_multitask_data( sentiment_filename, paraphrase_filename, similarity_filename, emotion_filename, split='train'):\n",
    "    '''This function loads the training datasets for the multitask dataset'''\n",
    "    sentiment_data = []\n",
    "    num_labels = {}\n",
    "\n",
    "    with open(sentiment_filename, 'r') as fp:\n",
    "        for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "            sent = record['sentence'].lower().strip()\n",
    "            sent_id = record['id'].lower().strip()\n",
    "            label = int(record['sentiment'].strip())\n",
    "            if label not in num_labels:\n",
    "                num_labels[label] = len(num_labels)\n",
    "            sentiment_data.append((sent, label,sent_id))\n",
    "\n",
    "    print(f\"Loaded {len(sentiment_data)} {split} examples from {sentiment_filename}\")\n",
    "\n",
    "    emotion_data= []\n",
    "    with open(emotion_filename, 'r') as fp:\n",
    "        for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "            sent = record['sentence'].lower().strip()\n",
    "            sent_id = record['id'].lower().strip()\n",
    "            label = int(record['sentiment'].strip())\n",
    "            if label not in num_labels:\n",
    "                num_labels[label] = len(num_labels)\n",
    "            emotion_data.append((sent, label,sent_id))\n",
    "\n",
    "    print(f\"Loaded {len(emotion_data)} {split} examples from {emotion_filename}\")\n",
    "\n",
    "    paraphrase_data = []\n",
    "    with open(paraphrase_filename, 'r') as fp:\n",
    "        for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "            try:\n",
    "                sent_id = record['id'].lower().strip()\n",
    "                paraphrase_data.append((preprocess_string(record['sentence1']), preprocess_string(record['sentence2']), int(float(record['is_duplicate'])),sent_id))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    print(f\"Loaded {len(paraphrase_data)} {split} examples from {paraphrase_filename}\")\n",
    "\n",
    "    similarity_data = []\n",
    "    with open(similarity_filename, 'r') as fp:\n",
    "        for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "            sent_id = record['id'].lower().strip()\n",
    "            similarity_data.append((preprocess_string(record['sentence1']), preprocess_string(record['sentence2']), float(record['similarity']),sent_id))\n",
    "\n",
    "    print(f\"Loaded {len(similarity_data)} {split} examples from {similarity_filename}\")\n",
    "    return sentiment_data, num_labels, paraphrase_data, similarity_data, emotion_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert class which helps in training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_MultiTask(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(Bert_MultiTask, self).__init__()\n",
    "        self.model = BertModel.from_pretrained(\"bert-base-uncased\", torch_dtype=torch.float16); self.model.to(\"cpu\")\n",
    "        self.tokenizer= BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        BERT_HIDDEN_SIZE = 768\n",
    "        \n",
    "        N_SENTIMENT_CLASSES = 5;    N_EMOTION_CLASSES= 14\n",
    "\n",
    "        # defining the linear layers for sentiment classification\n",
    "        self.dropout_sentiment = nn.ModuleList([nn.Dropout(config.hidden_dropout_prob) for _ in range(config.n_hidden_layers + 1)])\n",
    "        self.linear_sentiment = nn.ModuleList([nn.Linear(BERT_HIDDEN_SIZE, BERT_HIDDEN_SIZE, dtype=torch.float16) for _ in range(config.n_hidden_layers)] + [nn.Linear(BERT_HIDDEN_SIZE, N_SENTIMENT_CLASSES, dtype=torch.float16)])\n",
    "        self.last_linear_sentiment = None\n",
    "\n",
    "        # defining the layers for emotion detection\n",
    "        self.dropout_emotion = nn.ModuleList([nn.Dropout(config.hidden_dropout_prob) for _ in range(config.n_hidden_layers + 1)])\n",
    "        self.linear_emotion = nn.ModuleList([nn.Linear(BERT_HIDDEN_SIZE, BERT_HIDDEN_SIZE, dtype=torch.float16) for _ in range(config.n_hidden_layers)] + [nn.Linear(BERT_HIDDEN_SIZE, N_EMOTION_CLASSES, dtype=torch.float16)])\n",
    "        self.last_linear_emotion = None\n",
    "\n",
    "        # Add a linear layer for paraphrase detection\n",
    "        self.dropout_paraphrase = nn.ModuleList([nn.Dropout(config.hidden_dropout_prob) for _ in range(config.n_hidden_layers + 1)])\n",
    "        self.linear_paraphrase = nn.ModuleList([nn.Linear(BERT_HIDDEN_SIZE, BERT_HIDDEN_SIZE, dtype=torch.float16) for _ in range(config.n_hidden_layers)] + [nn.Linear(BERT_HIDDEN_SIZE, 1, dtype=torch.float16)])\n",
    "\n",
    "        # Add a linear layer for semantic textual similarity\n",
    "        self.dropout_similarity = nn.ModuleList([nn.Dropout(config.hidden_dropout_prob) for _ in range(config.n_hidden_layers + 1)])\n",
    "        self.linear_similarity = nn.ModuleList([nn.Linear(BERT_HIDDEN_SIZE, BERT_HIDDEN_SIZE,dtype=torch.float16) for _ in range(config.n_hidden_layers)] + [nn.Linear(BERT_HIDDEN_SIZE, 1,dtype=torch.float16)])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, task_id):\n",
    "        with torch.autocast(device_type='cpu', dtype=torch.float16):\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the [CLS] token embedding\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  \n",
    "        combined_embedding = cls_embedding\n",
    "        return combined_embedding\n",
    "    \n",
    "    def last_layers_sentiment(self, x):\n",
    "        for i in range(len(self.linear_sentiment) - 1):\n",
    "            x = self.dropout_sentiment[i](x)\n",
    "            x.to(torch.float16)\n",
    "            x = self.linear_sentiment[i](x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        x = self.dropout_sentiment[-1](x)\n",
    "        logits = self.linear_sentiment[-1](x)\n",
    "        return logits\n",
    "    \n",
    "    def predict_sentiment(self, input_ids, attention_mask):\n",
    "        x = self.forward(input_ids, attention_mask, task_id=0 )\n",
    "        x = self.last_layers_sentiment(x)\n",
    "        return x\n",
    "    \n",
    "    def last_layers_emotion(self, x):\n",
    "        for i in range(len(self.linear_emotion) - 1):\n",
    "            x = self.dropout_emotion[i](x)\n",
    "            x.to(torch.float16)\n",
    "            x = self.linear_emotion[i](x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        x = self.dropout_emotion[-1](x)\n",
    "        logits = self.linear_emotion[-1](x)\n",
    "        return logits\n",
    "    \n",
    "    def predict_emotion(self, input_ids, attention_mask):\n",
    "        x = self.forward(input_ids, attention_mask, task_id=3 )\n",
    "        x = self.last_layers_emotion(x)\n",
    "        return x\n",
    "\n",
    "    def get_similarity_paraphrase_embeddings(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2, task_id):\n",
    "        # Get [SEP] token ids\n",
    "        sep_token_id = torch.tensor([self.tokenizer.sep_token_id], dtype=torch.long, device=input_ids_1.device)\n",
    "        batch_sep_token_id = sep_token_id.repeat(input_ids_1.shape[0], 1)\n",
    "\n",
    "        # Concatenate the two sentences in: sent1 [SEP] sent2 [SEP]\n",
    "        input_id = torch.cat((input_ids_1, batch_sep_token_id, input_ids_2, batch_sep_token_id), dim=1)\n",
    "        attention_mask = torch.cat((attention_mask_1, torch.ones_like(batch_sep_token_id), attention_mask_2, torch.ones_like(batch_sep_token_id)), dim=1)\n",
    "        x = self.forward(input_id, attention_mask, task_id=task_id)\n",
    "        return x\n",
    "\n",
    "    def last_layers_paraphrase(self, x):\n",
    "        for i in range(len(self.linear_paraphrase) - 1):\n",
    "            x = self.dropout_paraphrase[i](x)\n",
    "            x = self.linear_paraphrase[i](x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        x = self.dropout_paraphrase[-1](x)\n",
    "        logits = self.linear_paraphrase[-1](x)\n",
    "        return logits\n",
    "\n",
    "    def predict_paraphrase(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2):\n",
    "        x = self.get_similarity_paraphrase_embeddings(input_ids_1, attention_mask_1, input_ids_2, attention_mask_2, task_id=1)\n",
    "        return self.last_layers_paraphrase(x)\n",
    "\n",
    "\n",
    "    def last_layers_similarity(self, x):\n",
    "        for i in range(len(self.linear_similarity) - 1):\n",
    "            x = self.dropout_similarity[i](x)\n",
    "            x = self.linear_similarity[i](x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        x = self.dropout_similarity[-1](x)\n",
    "        preds = self.linear_similarity[-1](x)\n",
    "        preds = torch.sigmoid(preds) * 4 + 1\n",
    "        return preds\n",
    "    \n",
    "    def predict_similarity(self,input_ids_1, attention_mask_1,input_ids_2, attention_mask_2):\n",
    "        x = self.get_similarity_paraphrase_embeddings(input_ids_1, attention_mask_1, input_ids_2, attention_mask_2, task_id=2)\n",
    "        return self.last_layers_similarity(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objects and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectsGroup:\n",
    "    def __init__(self, model, optimizer, scaler = None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scaler = scaler\n",
    "        self.loss_sum = 0\n",
    "\n",
    "class Scheduler:\n",
    "    def __init__(self, dataloaders, reset=True):\n",
    "        self.dataloaders = dataloaders\n",
    "        self.names = list(dataloaders.keys())\n",
    "        if reset: self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sst_iter = iter(self.dataloaders['sst'])\n",
    "        self.para_iter = iter(self.dataloaders['para'])\n",
    "        self.sts_iter = iter(self.dataloaders['sts'])\n",
    "        self.emt_iter = iter(self.dataloaders['emt'])\n",
    "        self.steps = {'sst': 0,  'para':0, 'sts':0, 'emt':0}\n",
    "\n",
    "    def get_SST_batch(self):\n",
    "        try:\n",
    "            return next(self.sst_iter)\n",
    "        except StopIteration:\n",
    "            self.sst_iter = cycle(self.dataloaders['sst'])\n",
    "            return next(self.sst_iter)\n",
    "\n",
    "    def get_EMT_batch(self):\n",
    "        try:\n",
    "            return next(self.emt_iter)\n",
    "        except StopIteration:\n",
    "            self.emt_iter = cycle(self.dataloaders['emt'])\n",
    "            return next(self.emt_iter)\n",
    "\n",
    "    def get_Paraphrase_batch(self):\n",
    "        try:\n",
    "            return next(self.para_iter)\n",
    "        except StopIteration:\n",
    "            self.para_iter = cycle(self.dataloaders['para'])\n",
    "            return next(self.para_iter)\n",
    "\n",
    "    def get_STS_batch(self):\n",
    "        try:\n",
    "            return next(self.sts_iter)\n",
    "        except StopIteration:\n",
    "            self.sts_iter = cycle(self.dataloaders['sts'])\n",
    "            return next(self.sts_iter)\n",
    "\n",
    "    def get_batch(self, name: str):\n",
    "        if name == \"sst\": return self.get_SST_batch()\n",
    "        elif name == \"para\": return self.get_Paraphrase_batch()\n",
    "        elif name == \"sts\": return self.get_STS_batch()\n",
    "        elif name == \"emt\": return self.get_EMT_batch()\n",
    "        raise ValueError(f\"Unknown batch name: {name}\")\n",
    "\n",
    "    def process_named_batch(self, objects_group: ObjectsGroup, args: dict, name: str, prev, val, apply_optimization: bool = True):\n",
    "        '''Processes a batch of data from the given dataset, and updates the model accordingly.'''\n",
    "        batch = self.get_batch(name)\n",
    "        process_fn, gradient_accumulations = None, 0\n",
    "        if name == \"sst\":\n",
    "            process_fn = process_sentiment_batch\n",
    "            gradient_accumulations = args['gradient_accumulations_sst']\n",
    "        elif name == \"para\":\n",
    "            process_fn = process_paraphrase_batch\n",
    "            gradient_accumulations = args['gradient_accumulations_para']\n",
    "        elif name == \"sts\":\n",
    "            process_fn = process_similarity_batch\n",
    "            gradient_accumulations = args['gradient_accumulations_sts']\n",
    "        elif name == \"emt\":\n",
    "            process_fn = process_emotion_batch\n",
    "            gradient_accumulations = args['gradient_accumulations_emt']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown batch name: {name}\")\n",
    "        \n",
    "        loss_of_batch = 0\n",
    "        for _ in range(gradient_accumulations):\n",
    "            loss_of_batch += process_fn(batch, objects_group, args)\n",
    "\n",
    "        self.steps[name] += 1\n",
    "        if apply_optimization: step_optimizer(objects_group, args, step=self.steps[name])\n",
    "        \n",
    "        if(torch.isnan(loss_of_batch).item()):\n",
    "            loss_of_batch= np.sum(prev) / val\n",
    "\n",
    "        return loss_of_batch\n",
    "\n",
    "\n",
    "class RoundRobinScheduler(Scheduler):\n",
    "    def __init__(self, dataloaders):\n",
    "        super().__init__(dataloaders, reset=False)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return super().reset()\n",
    "\n",
    "    def process_one_batch(self, epoch: int, num_epochs: int, objects_group: ObjectsGroup, args: dict):\n",
    "        name = self.names[self.index]\n",
    "        self.index = (self.index + 1) % len(self.names)\n",
    "        return name, self.process_named_batch(objects_group, args, name)\n",
    "\n",
    "def process_sentiment_batch(batch, objects_group: ObjectsGroup, args: dict):\n",
    "    device = 'cpu'\n",
    "    model, scaler = objects_group.model, objects_group.scaler\n",
    "\n",
    "    with torch.autocast(device_type='cpu', dtype=torch.float16):\n",
    "        b_ids, b_mask, b_labels = (batch['token_ids'], batch['attention_mask'], batch['labels'])\n",
    "        b_ids, b_mask, b_labels = b_ids.to(device), b_mask.to(device), b_labels.to(device)\n",
    "\n",
    "        embeddings = model.forward(b_ids, b_mask, task_id=0)\n",
    "        logits = model.last_layers_sentiment(embeddings)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, b_labels.view(-1), reduction='sum') / args['sst_batch_size']\n",
    "        loss_value = loss.item()        \n",
    "        objects_group.loss_sum += loss_value\n",
    "\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "def process_emotion_batch(batch, objects_group: ObjectsGroup, args: dict):\n",
    "    device = 'cpu'\n",
    "    model, scaler = objects_group.model, objects_group.scaler\n",
    "\n",
    "    with torch.autocast(device_type='cpu', dtype=torch.float16):\n",
    "        b_ids, b_mask, b_labels = (batch['token_ids'], batch['attention_mask'], batch['labels'])\n",
    "        b_ids, b_mask, b_labels = b_ids.to(device), b_mask.to(device), b_labels.to(device)\n",
    "\n",
    "        embeddings = model.forward(b_ids, b_mask, task_id=3)\n",
    "        logits = model.last_layers_emotion(embeddings)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, b_labels.view(-1), reduction='sum') / args['emt_batch_size']\n",
    "        loss_value = loss.item()        \n",
    "        objects_group.loss_sum += loss_value\n",
    "\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "def process_paraphrase_batch(batch, objects_group: ObjectsGroup, args: dict):\n",
    "    device = 'cpu'\n",
    "    model, scaler = objects_group.model, objects_group.scaler\n",
    "\n",
    "    with torch.autocast(device_type='cpu', dtype=torch.float16):\n",
    "        b_ids_1, b_mask_1, b_ids_2, b_mask_2, b_labels = (batch['token_ids_1'], batch['attention_mask_1'], batch['token_ids_2'], batch['attention_mask_2'], batch['labels'])\n",
    "        b_ids_1, b_mask_1, b_ids_2, b_mask_2, b_labels = b_ids_1.to(device), b_mask_1.to(device), b_ids_2.to(device), b_mask_2.to(device), b_labels.to(device)\n",
    "\n",
    "        embeddings = model.get_similarity_paraphrase_embeddings(b_ids_1, b_mask_1, b_ids_2, b_mask_2, task_id=1)\n",
    "        preds = model.last_layers_paraphrase(embeddings)\n",
    "        loss = F.binary_cross_entropy_with_logits(preds.view(-1), b_labels.float(), reduction='sum') / args['para_batch_size']\n",
    "        loss_value = loss.item()\n",
    "        objects_group.loss_sum += loss_value        \n",
    "        loss.backward()        \n",
    "        return loss\n",
    "\n",
    "def process_similarity_batch(batch, objects_group: ObjectsGroup, args: dict):\n",
    "    device = 'cpu'\n",
    "    model, scaler = objects_group.model, objects_group.scaler\n",
    "\n",
    "    with torch.autocast(device_type='cpu', dtype=torch.float16):\n",
    "        b_ids_1, b_mask_1, b_ids_2, b_mask_2, b_labels = (batch['token_ids_1'], batch['attention_mask_1'], batch['token_ids_2'], batch['attention_mask_2'], batch['labels'])\n",
    "        b_ids_1, b_mask_1, b_ids_2, b_mask_2, b_labels = b_ids_1.to(device), b_mask_1.to(device), b_ids_2.to(device), b_mask_2.to(device), b_labels.to(device)\n",
    "        embeddings = model.get_similarity_paraphrase_embeddings(b_ids_1, b_mask_1, b_ids_2, b_mask_2, task_id=2)\n",
    "        preds = model.last_layers_similarity(embeddings)\n",
    "        loss = F.mse_loss(preds.view(-1), b_labels.view(-1), reduction='sum') / args['sts_batch_size']\n",
    "        loss_value = loss.item()\n",
    "        objects_group.loss_sum += loss_value\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "def step_optimizer(objects_group: ObjectsGroup, args: dict, step: int, total_nb_batches = None):\n",
    "    optimizer, scaler = objects_group.optimizer, objects_group.scaler\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    loss_value = objects_group.loss_sum\n",
    "    objects_group.loss_sum = 0\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value= 4269\n",
    "seed_everything(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args= {\n",
    "    'sst_file': './train_data/train_sentiment.csv',\n",
    "    'para_file': './train_data/train_paraphase.csv',\n",
    "    'sts_file':  './train_data/train_similarity.csv',\n",
    "    'emt_file': './train_data/train_emotion.csv',\n",
    "    'para_batch_size': 2,\n",
    "    'sst_batch_size': 2,\n",
    "    'sts_batch_size': 2,\n",
    "    'emt_batch_size':2,\n",
    "    'hidden_layers': 2,\n",
    "    'hidden_drp_prob': 0.2,\n",
    "    'lr': 1e-6,\n",
    "    'epochs': 6,\n",
    "    'patience': 0.2,\n",
    "    'option': 'individual_pertrin',\n",
    "    'num_batches_per_epoch':2,\n",
    "    'gradient_accumulations_sst':3,\n",
    "    'gradient_accumulations_sts':3,\n",
    "    'gradient_accumulations_para':3,\n",
    "    'gradient_accumulations_emt': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 70 train examples from ./train_data/train_sentiment.csv\n",
      "Loaded 70 train examples from ./train_data/train_emotion.csv\n",
      "Loaded 70 train examples from ./train_data/train_paraphase.csv\n",
      "Loaded 70 train examples from ./train_data/train_similarity.csv\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "sst_train_data, num_labels,para_train_data, sts_train_data, emt_train_data= load_multitask_data(args['sst_file'],args['para_file'],args['sts_file'], args['emt_file'],  split ='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment data loader\n",
    "sst_train_data = SentenceClassificationDataset(sst_train_data)\n",
    "sst_train_dataloader = DataLoader(sst_train_data, shuffle=True, batch_size= args['sst_batch_size'], collate_fn=sst_train_data.collate_fn)\n",
    "\n",
    "# Emotion data loader\n",
    "emt_train_data = SentenceClassificationDataset(emt_train_data)\n",
    "emt_train_dataloader = DataLoader(emt_train_data, shuffle=True, batch_size= args['emt_batch_size'], collate_fn=emt_train_data.collate_fn)\n",
    "\n",
    "# Paraphrase data loader\n",
    "para_train_data = SentencePairDataset(para_train_data)\n",
    "para_train_dataloader = DataLoader(para_train_data, shuffle=True, batch_size= args['para_batch_size'],collate_fn=para_train_data.collate_fn)\n",
    "\n",
    "# Similarity detection data loader\n",
    "sts_train_data = SentencePairDataset(sts_train_data, isRegression=True)\n",
    "sts_train_dataloader = DataLoader(sts_train_data, shuffle=True, batch_size=args['sts_batch_size'],collate_fn=sts_train_data.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'hidden_dropout_prob': args['hidden_drp_prob'],'num_labels': 5,'hidden_size': 768,'data_dir': '.','option': args['option'],'n_hidden_layers': args['hidden_layers']}\n",
    "config = SimpleNamespace(**config)\n",
    "model = Bert_MultiTask(config).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=args['lr'])\n",
    "scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_group = ObjectsGroup(model, optimizer, scaler)\n",
    "args['device'] = device\n",
    "dataloaders = {'sst': sst_train_dataloader, 'para': para_train_dataloader, 'sts': sts_train_dataloader, 'emt':emt_train_dataloader}\n",
    "scheduler = RoundRobinScheduler(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst epoch 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst epoch 0: 100%|██████████| 2/2 [02:59<00:00, 89.84s/it]\n",
      "sts epoch 0: 100%|██████████| 2/2 [02:40<00:00, 80.48s/it]\n",
      "para epoch 0: 100%|██████████| 2/2 [04:42<00:00, 141.21s/it]\n",
      "emt epoch 0: 100%|██████████| 2/2 [01:57<00:00, 58.95s/it]\n",
      "sst epoch 1: 100%|██████████| 2/2 [04:00<00:00, 120.21s/it]\n",
      "sts epoch 1: 100%|██████████| 2/2 [02:15<00:00, 67.53s/it]\n",
      "para epoch 1: 100%|██████████| 2/2 [03:29<00:00, 104.69s/it]\n",
      "emt epoch 1: 100%|██████████| 2/2 [02:33<00:00, 76.79s/it]\n",
      "sst epoch 2: 100%|██████████| 2/2 [15:55<00:00, 477.76s/it]\n",
      "sts epoch 2: 100%|██████████| 2/2 [02:04<00:00, 62.43s/it]\n",
      "para epoch 2: 100%|██████████| 2/2 [03:51<00:00, 115.63s/it]\n",
      "emt epoch 2: 100%|██████████| 2/2 [01:36<00:00, 48.34s/it]\n",
      "sst epoch 3: 100%|██████████| 2/2 [04:12<00:00, 126.27s/it]\n",
      "sts epoch 3: 100%|██████████| 2/2 [01:48<00:00, 54.23s/it]\n",
      "para epoch 3: 100%|██████████| 2/2 [04:36<00:00, 138.17s/it]\n",
      "emt epoch 3: 100%|██████████| 2/2 [01:55<00:00, 57.99s/it]\n",
      "sst epoch 4: 100%|██████████| 2/2 [03:50<00:00, 115.46s/it]\n",
      "sts epoch 4: 100%|██████████| 2/2 [02:08<00:00, 64.19s/it]\n",
      "para epoch 4: 100%|██████████| 2/2 [03:01<00:00, 90.75s/it] \n",
      "emt epoch 4: 100%|██████████| 2/2 [01:50<00:00, 55.46s/it]\n",
      "sst epoch 5: 100%|██████████| 2/2 [04:58<00:00, 149.22s/it]\n",
      "sts epoch 5: 100%|██████████| 2/2 [02:21<00:00, 70.84s/it]\n",
      "para epoch 5: 100%|██████████| 2/2 [04:37<00:00, 138.88s/it]\n",
      "emt epoch 5: 100%|██████████| 2/2 [01:14<00:00, 37.21s/it]\n"
     ]
    }
   ],
   "source": [
    "total_loss = {'sst': [], 'para': [], 'sts': [], 'emt': []}\n",
    "n_batches = 0;  lva=0;t1=1\n",
    "num_batches_per_epoch = args['num_batches_per_epoch'] if args['num_batches_per_epoch'] > 0 else len(sst_train_dataloader)\n",
    "infos = {'sst': {'num_batches': num_batches_per_epoch,  'best_dev_acc': 0, 'best_model': None, 'layer': model.linear_sentiment, 'optimizer': AdamW(model.parameters(), lr= args['lr']), \"last_improv\": -1, 'first': True, 'first_loss': True}, \n",
    "        'para': {'num_batches': num_batches_per_epoch,  'best_dev_acc': 0, 'best_model': None, 'layer': model.linear_paraphrase, 'optimizer': AdamW(model.parameters(), lr=args['lr']), \"last_improv\": -1, 'first': True, 'first_loss': True},\n",
    "        'sts':  {'num_batches': num_batches_per_epoch, 'best_dev_acc': 0, 'best_model': None, 'layer': model.linear_similarity, 'optimizer': AdamW(model.parameters(), lr=args['lr']), \"last_improv\": -1, 'first': True, 'first_loss': True},\n",
    "        'emt':  {'num_batches': num_batches_per_epoch, 'best_dev_acc': 0, 'best_model': None, 'layer': model.linear_emotion, 'optimizer': AdamW(model.parameters(), lr=args['lr']), \"last_improv\": -1, 'first': True, 'first_loss': True}}\n",
    "\n",
    "total_num_batches = {'sst': 0, 'para': 0, 'sts': 0, 'emt': 0}\n",
    "for epoch in range(args['epochs']):\n",
    "    for task in ['sst', 'sts', 'para', 'emt']:\n",
    "        model.train()\n",
    "        objects_group.optimizer = infos[task]['optimizer']\n",
    "        for i in tqdm(range(infos[task]['num_batches']), desc=task + ' epoch ' + str(epoch), smoothing=0):\n",
    "            \n",
    "            loss = scheduler.process_named_batch(name=task, objects_group=objects_group, args=args, prev= total_loss[task], val=lva)\n",
    "            total_loss[task].append(float(loss.item()))\n",
    "\n",
    "            total_num_batches[task] += 1\n",
    "            n_batches += 1\n",
    "            lva+= t1; t1+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    model, 'multimodel.pt'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
