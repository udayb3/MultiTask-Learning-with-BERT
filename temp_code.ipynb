{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random, numpy as np, argparse, sys, re, os\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast\n",
    "from contextlib import nullcontext\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import copy\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the random seed\n",
    "def seed_everything(seed= 10002):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This module contains our Dataset classes and functions to load the 3 datasets we're using.\n",
    "You should only need to call load_multitask_data to get the training and dev examples\n",
    "to train your model.\n",
    "'''\n",
    "\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def preprocess_string(s):\n",
    "    '''Preprocesses a string by lowercasing it and adding spaces around punctuation.'''\n",
    "    return ' '.join(s.lower().replace('.', ' .').replace('?', ' ?').replace(',', ' ,').replace('\\'', ' \\'').split())\n",
    "\n",
    "\n",
    "class SentenceClassificationDataset(Dataset):\n",
    "    '''This class is a wrapper around the dataset with one sentence inputs that we will use to train our model.\n",
    "    (ie. the SST dataset)'''\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def pad_data(self, data):\n",
    "        '''This function pads the data to the max length of the batch.'''\n",
    "        sents = [x[0] for x in data]\n",
    "        labels = [x[1] for x in data]\n",
    "        sent_ids = [x[2] for x in data]\n",
    "\n",
    "        encoding = self.tokenizer(sents, return_tensors='pt', padding=True, truncation=True)\n",
    "        token_ids = torch.LongTensor(encoding['input_ids'])\n",
    "        attention_mask = torch.LongTensor(encoding['attention_mask'])\n",
    "        labels = torch.LongTensor(labels)\n",
    "\n",
    "        return token_ids, attention_mask, labels, sents, sent_ids\n",
    "\n",
    "    def collate_fn(self, all_data):\n",
    "        token_ids, attention_mask, labels, sents, sent_ids= self.pad_data(all_data)\n",
    "\n",
    "        batched_data = {\n",
    "                'token_ids': token_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels,\n",
    "                'sents': sents,\n",
    "                'sent_ids': sent_ids\n",
    "            }\n",
    "\n",
    "        return batched_data\n",
    "\n",
    "\n",
    "class SentenceClassificationTestDataset(Dataset):\n",
    "    '''This class is a wrapper around the dataset with one sentence inputs that we will use to test our model.\n",
    "    (ie. the SST dataset)'''\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def pad_data(self, data):\n",
    "        '''This function pads the data to the max length of the batch.'''\n",
    "        sents = [x[0] for x in data]\n",
    "        sent_ids = [x[1] for x in data]\n",
    "\n",
    "        encoding = self.tokenizer(sents, return_tensors='pt', padding=True, truncation=True)\n",
    "        token_ids = torch.LongTensor(encoding['input_ids'])\n",
    "        attention_mask = torch.LongTensor(encoding['attention_mask'])\n",
    "\n",
    "        return token_ids, attention_mask, sents, sent_ids\n",
    "\n",
    "    def collate_fn(self, all_data):\n",
    "        token_ids, attention_mask, sents, sent_ids= self.pad_data(all_data)\n",
    "\n",
    "        batched_data = {\n",
    "                'token_ids': token_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'sents': sents,\n",
    "                'sent_ids': sent_ids\n",
    "            }\n",
    "\n",
    "        return batched_data\n",
    "\n",
    "\n",
    "class SentencePairDataset(Dataset):\n",
    "    '''This class is a wrapper around the dataset with pair sentences that we will use to train our model.\n",
    "    (ie. A class for handling the SemEval and Quora datasets.)'''\n",
    "    def __init__(self, dataset, isRegression =False):\n",
    "        self.dataset = dataset\n",
    "        # self.p = args\n",
    "        self.isRegression = isRegression\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def pad_data(self, data):\n",
    "        '''This function pads the data to the max length of the batch.'''\n",
    "        sent1 = [x[0] for x in data]\n",
    "        sent2 = [x[1] for x in data]\n",
    "        labels = [x[2] for x in data]\n",
    "        sent_ids = [x[3] for x in data]\n",
    "\n",
    "        encoding1 = self.tokenizer(sent1, return_tensors='pt', padding=True, truncation=True)\n",
    "        encoding2 = self.tokenizer(sent2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        token_ids = torch.LongTensor(encoding1['input_ids'])\n",
    "        attention_mask = torch.LongTensor(encoding1['attention_mask'])\n",
    "        token_type_ids = torch.LongTensor(encoding1['token_type_ids'])\n",
    "\n",
    "        token_ids2 = torch.LongTensor(encoding2['input_ids'])\n",
    "        attention_mask2 = torch.LongTensor(encoding2['attention_mask'])\n",
    "        token_type_ids2 = torch.LongTensor(encoding2['token_type_ids'])\n",
    "        if self.isRegression:\n",
    "            labels = torch.FloatTensor(labels)\n",
    "        else:\n",
    "            labels = torch.LongTensor(labels)\n",
    "            \n",
    "\n",
    "        return (token_ids, token_type_ids, attention_mask,\n",
    "                token_ids2, token_type_ids2, attention_mask2,\n",
    "                labels,sent_ids)\n",
    "\n",
    "    def collate_fn(self, all_data):\n",
    "        (token_ids, token_type_ids, attention_mask,\n",
    "         token_ids2, token_type_ids2, attention_mask2,\n",
    "         labels, sent_ids) = self.pad_data(all_data)\n",
    "\n",
    "        batched_data = {\n",
    "                'token_ids_1': token_ids,\n",
    "                'token_type_ids_1': token_type_ids,\n",
    "                'attention_mask_1': attention_mask,\n",
    "                'token_ids_2': token_ids2,\n",
    "                'token_type_ids_2': token_type_ids2,\n",
    "                'attention_mask_2': attention_mask2,\n",
    "                'labels': labels,\n",
    "                'sent_ids': sent_ids\n",
    "            }\n",
    "\n",
    "        return batched_data\n",
    "\n",
    "\n",
    "class SentencePairTestDataset(Dataset):\n",
    "    '''This class is a wrapper around the dataset with pair sentences that we will use to test our model.\n",
    "    (ie. A class for handling the SemEval and Quora datasets.)'''\n",
    "    def __init__(self, dataset, tokenizer, args):\n",
    "        self.dataset = dataset\n",
    "        self.p = args\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def pad_data(self, data):\n",
    "        '''This function pads the data to the max length of the batch'''\n",
    "        sent1 = [x[0] for x in data]\n",
    "        sent2 = [x[1] for x in data]\n",
    "        sent_ids = [x[2] for x in data]\n",
    "\n",
    "        encoding1 = self.tokenizer(sent1, return_tensors='pt', padding=True, truncation=True)\n",
    "        encoding2 = self.tokenizer(sent2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        token_ids = torch.LongTensor(encoding1['input_ids'])\n",
    "        attention_mask = torch.LongTensor(encoding1['attention_mask'])\n",
    "        token_type_ids = torch.LongTensor(encoding1['token_type_ids'])\n",
    "\n",
    "        token_ids2 = torch.LongTensor(encoding2['input_ids'])\n",
    "        attention_mask2 = torch.LongTensor(encoding2['attention_mask'])\n",
    "        token_type_ids2 = torch.LongTensor(encoding2['token_type_ids'])\n",
    "\n",
    "\n",
    "        return (token_ids, token_type_ids, attention_mask,\n",
    "                token_ids2, token_type_ids2, attention_mask2,\n",
    "               sent_ids)\n",
    "\n",
    "    def collate_fn(self, all_data):\n",
    "        (token_ids, token_type_ids, attention_mask,\n",
    "         token_ids2, token_type_ids2, attention_mask2,\n",
    "         sent_ids) = self.pad_data(all_data)\n",
    "\n",
    "        batched_data = {\n",
    "                'token_ids_1': token_ids,\n",
    "                'token_type_ids_1': token_type_ids,\n",
    "                'attention_mask_1': attention_mask,\n",
    "                'token_ids_2': token_ids2,\n",
    "                'token_type_ids_2': token_type_ids2,\n",
    "                'attention_mask_2': attention_mask2,\n",
    "                'sent_ids': sent_ids\n",
    "            }\n",
    "\n",
    "        return batched_data\n",
    "\n",
    "\n",
    "def load_multitask_test_data():\n",
    "    '''This function loads the test datasets for the multitask dataset.'''\n",
    "    paraphrase_filename = f'data/quora-test.csv'\n",
    "    sentiment_filename = f'data/ids-sst-test.txt'\n",
    "    similarity_filename = f'data/sts-test.csv'\n",
    "\n",
    "    sentiment_data = []\n",
    "\n",
    "    with open(sentiment_filename, 'r') as fp:\n",
    "        for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "            sent = record['sentence'].lower().strip()\n",
    "            sentiment_data.append(sent)\n",
    "\n",
    "    print(f\"Loaded {len(sentiment_data)} test examples from {sentiment_filename}\")\n",
    "\n",
    "    paraphrase_data = []\n",
    "    with open(paraphrase_filename, 'r') as fp:\n",
    "        for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "            #if record['split'] != split:\n",
    "            #    continue\n",
    "            paraphrase_data.append((preprocess_string(record['sentence1']),\n",
    "                                    preprocess_string(record['sentence2']),\n",
    "                                    ))\n",
    "\n",
    "    print(f\"Loaded {len(paraphrase_data)} test examples from {paraphrase_filename}\")\n",
    "\n",
    "    similarity_data = []\n",
    "    with open(similarity_filename, 'r') as fp:\n",
    "        for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "            similarity_data.append((preprocess_string(record['sentence1']),\n",
    "                                    preprocess_string(record['sentence2']),\n",
    "                                    ))\n",
    "\n",
    "    print(f\"Loaded {len(similarity_data)} test examples from {similarity_filename}\")\n",
    "\n",
    "    return sentiment_data, paraphrase_data, similarity_data\n",
    "\n",
    "\n",
    "\n",
    "def load_multitask_data( sentiment_filename, paraphrase_filename, similarity_filename,split='train'):\n",
    "    '''This function loads the training datasets for the multitask dataset'''\n",
    "    sentiment_data = []\n",
    "    num_labels = {}\n",
    "    if split == 'test':\n",
    "        with open(sentiment_filename, 'r') as fp:\n",
    "            for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "                sent = record['sentence'].lower().strip()\n",
    "                sent_id = record['id'].lower().strip()\n",
    "                sentiment_data.append((sent,sent_id))\n",
    "    else:\n",
    "        with open(sentiment_filename, 'r') as fp:\n",
    "            for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "                sent = record['sentence'].lower().strip()\n",
    "                sent_id = record['id'].lower().strip()\n",
    "                label = int(record['sentiment'].strip())\n",
    "                if label not in num_labels:\n",
    "                    num_labels[label] = len(num_labels)\n",
    "                sentiment_data.append((sent, label,sent_id))\n",
    "\n",
    "    print(f\"Loaded {len(sentiment_data)} {split} examples from {sentiment_filename}\")\n",
    "\n",
    "    paraphrase_data = []\n",
    "    if split == 'test':\n",
    "        with open(paraphrase_filename, 'r') as fp:\n",
    "            for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "                sent_id = record['id'].lower().strip()\n",
    "                paraphrase_data.append((preprocess_string(record['sentence1']), preprocess_string(record['sentence2']), sent_id))\n",
    "    else:\n",
    "        with open(paraphrase_filename, 'r') as fp:\n",
    "            for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "                try:\n",
    "                    sent_id = record['id'].lower().strip()\n",
    "                    paraphrase_data.append((preprocess_string(record['sentence1']), preprocess_string(record['sentence2']), int(float(record['is_duplicate'])),sent_id))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    print(f\"Loaded {len(paraphrase_data)} {split} examples from {paraphrase_filename}\")\n",
    "\n",
    "    similarity_data = []\n",
    "    if split == 'test':\n",
    "        with open(similarity_filename, 'r') as fp:\n",
    "            for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "                sent_id = record['id'].lower().strip()\n",
    "                similarity_data.append((preprocess_string(record['sentence1']), preprocess_string(record['sentence2']) ,sent_id))\n",
    "    else:\n",
    "        with open(similarity_filename, 'r') as fp:\n",
    "            for record in csv.DictReader(fp,delimiter = '\\t'):\n",
    "                sent_id = record['id'].lower().strip()\n",
    "                similarity_data.append((preprocess_string(record['sentence1']), preprocess_string(record['sentence2']), float(record['similarity']),sent_id))\n",
    "\n",
    "    print(f\"Loaded {len(similarity_data)} {split} examples from {similarity_filename}\")\n",
    "\n",
    "    return sentiment_data, num_labels, paraphrase_data, similarity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_MultiTask(nn.Module):\n",
    "    \"\"\"\"\n",
    "    config: {\n",
    "        'hidden_dropout_prob':\n",
    "        'option':\n",
    "        'num_hidden_lrs'\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self,config):\n",
    "        super(Bert_MultiTask, self).__init__()\n",
    "        self.model = BertModel.from_pretrained(\"bert-base-uncased\", torch_dtype=torch.float16, attn_implementation=\"sdpa\");  \n",
    "        self.tokenizer= BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        BERT_HIDDEN_SIZE = 768\n",
    "        \n",
    "        N_SENTIMENT_CLASSES = 5\n",
    "        N_STS_CLASSES = 6\n",
    "\n",
    "        \n",
    "        # setting the model for finetuning or full training\n",
    "        for param in self.model.parameters():\n",
    "            if config.option == 'finetune':\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # defining the lineear layers for sentiment classification\n",
    "        self.dropout_sentiment = nn.ModuleList([nn.Dropout(config.hidden_dropout_prob) for _ in range(config.n_hidden_layers + 1)])\n",
    "        self.linear_sentiment = nn.ModuleList([nn.Linear(BERT_HIDDEN_SIZE, BERT_HIDDEN_SIZE, dtype=torch.float16) for _ in range(config.n_hidden_layers)] + [nn.Linear(BERT_HIDDEN_SIZE, N_SENTIMENT_CLASSES, dtype=torch.float16)])\n",
    "        self.last_linear_sentiment = None\n",
    "\n",
    "        # Step 3: Add a linear layer for paraphrase detection\n",
    "        self.dropout_paraphrase = nn.ModuleList([nn.Dropout(config.hidden_dropout_prob) for _ in range(config.n_hidden_layers + 1)])\n",
    "        self.linear_paraphrase = nn.ModuleList([nn.Linear(BERT_HIDDEN_SIZE, BERT_HIDDEN_SIZE, dtype=torch.float16) for _ in range(config.n_hidden_layers)] + [nn.Linear(BERT_HIDDEN_SIZE, 1, dtype=torch.float16)])\n",
    "\n",
    "        # Step 4: Add a linear layer for semantic textual similarity\n",
    "        # This is a regression task, so the output should be a single number\n",
    "        self.dropout_similarity = nn.ModuleList([nn.Dropout(config.hidden_dropout_prob) for _ in range(config.n_hidden_layers + 1)])\n",
    "        self.linear_similarity = nn.ModuleList([nn.Linear(BERT_HIDDEN_SIZE, BERT_HIDDEN_SIZE,dtype=torch.float16) for _ in range(config.n_hidden_layers)] + [nn.Linear(BERT_HIDDEN_SIZE, 1,dtype=torch.float16)])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, task_id):\n",
    "        # gives embeddings for the batch of sentences\n",
    "        # The final BERT embedding is the hidden state of [CLS] token (the first token)\n",
    "        # Here, you can start by just returning the embeddings straight from BERT.\n",
    "        # When thinking of improvements, you can later try modifying this\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the [CLS] token embedding (the first token's hidden state)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # (batch_size, hidden_size)\n",
    "\n",
    "        # Combine CLS embedding with task embedding (e.g., addition, concatenation)\n",
    "        combined_embedding = cls_embedding\n",
    "        return combined_embedding\n",
    "    \n",
    "    def last_layers_sentiment(self, x):\n",
    "        \"\"\"Given a batch of sentences embeddings, outputs logits for classifying sentiment.\"\"\"\n",
    "        for i in range(len(self.linear_sentiment) - 1):\n",
    "            x = self.dropout_sentiment[i](x)\n",
    "            x = self.linear_sentiment[i](x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        x = self.dropout_sentiment[-1](x)\n",
    "        logits = self.linear_sentiment[-1](x)\n",
    "        # logits = F.softmax(logits, dim=1)\n",
    "        return logits\n",
    "    \n",
    "    def predict_sentiment(self, input_ids, attention_mask):\n",
    "        '''Given a batch of sentences, outputs logits for classifying sentiment.\n",
    "        There are 5 sentiment classes:\n",
    "        (0 - negative, 1- somewhat negative, 2- neutral, 3- somewhat positive, 4- positive)\n",
    "        Thus, your output should contain 5 logits for each sentence.\n",
    "        '''\n",
    "        x = self.forward(input_ids, attention_mask, task_id=0 )\n",
    "        x = self.last_layers_sent(x)\n",
    "        return x\n",
    "\n",
    "    def get_similarity_paraphrase_embeddings(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2, task_id):\n",
    "        '''Given a batch of pairs of sentences, get the BERT embeddings.'''\n",
    "        # Get [SEP] token ids\n",
    "        sep_token_id = torch.tensor([self.tokenizer.sep_token_id], dtype=torch.long, device=input_ids_1.device)\n",
    "        batch_sep_token_id = sep_token_id.repeat(input_ids_1.shape[0], 1)\n",
    "\n",
    "        # Concatenate the two sentences in: sent1 [SEP] sent2 [SEP]\n",
    "        input_id = torch.cat((input_ids_1, batch_sep_token_id, input_ids_2, batch_sep_token_id), dim=1)\n",
    "        attention_mask = torch.cat((attention_mask_1, torch.ones_like(batch_sep_token_id), attention_mask_2, torch.ones_like(batch_sep_token_id)), dim=1)\n",
    "\n",
    "        # Get the BERT embeddings\n",
    "        x = self.forward(input_id, attention_mask, task_id=task_id)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def last_layers_paraphrase(self, x):\n",
    "        \"\"\"Given a batch of pairs of sentences embedding, outputs logits for predicting whether they are paraphrases.\"\"\"\n",
    "        #Step 2: Hidden layers\n",
    "        for i in range(len(self.linear_paraphrase) - 1):\n",
    "            x = self.dropout_paraphrase[i](x)\n",
    "            x = self.linear_paraphrase[i](x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        # Step 3: Final layer\n",
    "        x = self.dropout_paraphrase[-1](x)\n",
    "        logits = self.linear_paraphrase[-1](x)\n",
    "        # logits = torch.sigmoid(logits)\n",
    "        return logits\n",
    "\n",
    "    def predict_paraphrase(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit corresponding to how similar they are.\n",
    "        Note that your output should be unnormalized (a logit).\n",
    "        '''\n",
    "        # Step 1: Get the BERT embeddings\n",
    "        x = self.get_similarity_paraphrase_embeddings(input_ids_1, attention_mask_1, input_ids_2, attention_mask_2, task_id=1)\n",
    "        return self.last_layers_paraphrase(x)\n",
    "\n",
    "\n",
    "    def last_layers_similarity(self, x):\n",
    "        \"\"\"Given a batch of pairs of sentences embeddings, outputs logits for predicting how similar they are.\"\"\"\n",
    "        # Step 3: Hidden layers\n",
    "        for i in range(len(self.linear_similarity) - 1):\n",
    "            x = self.dropout_similarity[i](x)\n",
    "            x = self.linear_similarity[i](x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        # Step 4: Final layer\n",
    "        x = self.dropout_similarity[-1](x)\n",
    "        preds = self.linear_similarity[-1](x)\n",
    "        # preds = torch.sigmoid(preds) * 6 - 0.5 # Scale to [-0.5, 5.5]\n",
    "\n",
    "        # # If we are evaluating, then we cap the predictions to the range [0, 5]\n",
    "        # if not self.training:\n",
    "        #     preds = torch.clamp(preds, 0, 5)\n",
    "        return preds\n",
    "    \n",
    "    def predict_similarity(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit corresponding to how similar they are.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "        # Step 1 : Get the BERT embeddings\n",
    "        x = self.get_similarity_paraphrase_embeddings(input_ids_1, attention_mask_1, input_ids_2, attention_mask_2, task_id=2)\n",
    "        return self.last_layers_similarity(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectsGroup:\n",
    "\n",
    "    def __init__(self, model, optimizer, scaler = None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scaler = scaler\n",
    "        self.loss_sum = 0\n",
    "\n",
    "class Scheduler:\n",
    "    '''A class to manage the learning rate scheduler.'''\n",
    "\n",
    "    def __init__(self, dataloaders, reset=True):\n",
    "        self.dataloaders = dataloaders\n",
    "        self.names = list(dataloaders.keys())\n",
    "        if reset: self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sst_iter = iter(self.dataloaders['sst'])\n",
    "        # self.para_iter = iter(self.dataloaders['para'])\n",
    "        # self.sts_iter = iter(self.dataloaders['sts'])\n",
    "        self.steps = {'sst': 0}\n",
    "\n",
    "    def get_SST_batch(self):\n",
    "        try:\n",
    "            return next(self.sst_iter)\n",
    "        except StopIteration:\n",
    "            self.sst_iter = cycle(self.dataloaders['sst'])\n",
    "            return next(self.sst_iter)\n",
    "\n",
    "    def get_Paraphrase_batch(self):\n",
    "        try:\n",
    "            return next(self.para_iter)\n",
    "        except StopIteration:\n",
    "            self.para_iter = cycle(self.dataloaders['para'])\n",
    "            return next(self.para_iter)\n",
    "\n",
    "    def get_STS_batch(self):\n",
    "        try:\n",
    "            return next(self.sts_iter)\n",
    "        except StopIteration:\n",
    "            self.sts_iter = cycle(self.dataloaders['sts'])\n",
    "            return next(self.sts_iter)\n",
    "\n",
    "    def get_batch(self, name: str):\n",
    "        if name == \"sst\": return self.get_SST_batch()\n",
    "        # elif name == \"para\": return self.get_Paraphrase_batch()\n",
    "        # elif name == \"sts\": return self.get_STS_batch()\n",
    "        raise ValueError(f\"Unknown batch name: {name}\")\n",
    "\n",
    "    def process_named_batch(self, objects_group: ObjectsGroup, args: dict, name: str, apply_optimization: bool = True):\n",
    "        '''Processes a batch of data from the given dataset, and updates the model accordingly.'''\n",
    "        batch = self.get_batch(name)\n",
    "        process_fn, gradient_accumulations = None, 0\n",
    "        if name == \"sst\":\n",
    "            process_fn = process_sentiment_batch\n",
    "            gradient_accumulations = args.gradient_accumulations_sst\n",
    "        elif name == \"para\":\n",
    "            process_fn = process_paraphrase_batch\n",
    "            gradient_accumulations = args.gradient_accumulations_para\n",
    "        elif name == \"sts\":\n",
    "            process_fn = process_similarity_batch\n",
    "            gradient_accumulations = args.gradient_accumulations_sts\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown batch name: {name}\")\n",
    "        \n",
    "        # Process the batch\n",
    "        loss_of_batch = 0\n",
    "        for _ in range(gradient_accumulations):\n",
    "            loss_of_batch += process_fn(batch, objects_group, args)\n",
    "\n",
    "        # Update the model\n",
    "        self.steps[name] += 1\n",
    "        if apply_optimization: step_optimizer(objects_group, args, step=self.steps[name])\n",
    "\n",
    "        return loss_of_batch\n",
    "\n",
    "\n",
    "class RoundRobinScheduler(Scheduler):\n",
    "    '''A scheduler that processes batches in a round-robin fashion.'''\n",
    "    def __init__(self, dataloaders):\n",
    "        super().__init__(dataloaders, reset=False)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        return super().reset()\n",
    "\n",
    "    def process_one_batch(self, epoch: int, num_epochs: int, objects_group: ObjectsGroup, args: dict):\n",
    "        name = self.names[self.index]\n",
    "        self.index = (self.index + 1) % len(self.names)\n",
    "        return name, self.process_named_batch(objects_group, args, name)\n",
    "\n",
    "def process_sentiment_batch(batch, objects_group: ObjectsGroup, args: dict):\n",
    "    '''This function processes a batch of SST data. It takes as input a batch of data, a group of objects (model, optimizer, scheduler, etc.), \n",
    "    and the arguments. It returns the loss of the batch.'''\n",
    "    device = args.device\n",
    "    model, scaler = objects_group.model, objects_group.scaler\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        b_ids, b_mask, b_labels = (batch['token_ids'], batch['attention_mask'], batch['labels'])\n",
    "        b_ids, b_mask, b_labels = b_ids.to(device), b_mask.to(device), b_labels.to(device)\n",
    "\n",
    "        embeddings = model.forward(b_ids, b_mask, task_id=0)\n",
    "        logits = model.last_layers_sentiment(embeddings)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, b_labels.view(-1), reduction='sum') / args.batch_size\n",
    "        loss_value = loss.item()\n",
    "        \n",
    "        objects_group.loss_sum += loss_value\n",
    "\n",
    "        if args.projection == \"none\":\n",
    "            if args.use_amp: scaler.scale(loss).backward()\n",
    "            else: loss.backward()\n",
    "        return loss\n",
    "\n",
    "\n",
    "def process_paraphrase_batch(batch, objects_group: ObjectsGroup, args: dict):\n",
    "    '''This function processes a batch of paraphrase data. It takes as input a batch of data, \n",
    "    a group of objects (model, optimizer, scheduler, etc.), and the arguments. It returns the loss of the batch.'''\n",
    "    device = args.device\n",
    "    model, scaler = objects_group.model, objects_group.scaler\n",
    "\n",
    "    with autocast('cuda') if args.use_amp else nullcontext():\n",
    "        b_ids_1, b_mask_1, b_ids_2, b_mask_2, b_labels = (batch['token_ids_1'], batch['attention_mask_1'], batch['token_ids_2'], batch['attention_mask_2'], batch['labels'])\n",
    "        b_ids_1, b_mask_1, b_ids_2, b_mask_2, b_labels = b_ids_1.to(device), b_mask_1.to(device), b_ids_2.to(device), b_mask_2.to(device), b_labels.to(device)\n",
    "\n",
    "        embeddings = model.get_similarity_paraphrase_embeddings(b_ids_1, b_mask_1, b_ids_2, b_mask_2, task_id=1)\n",
    "        preds = model.last_layers_paraphrase(embeddings)\n",
    "        loss = F.binary_cross_entropy_with_logits(preds.view(-1), b_labels.float(), reduction='sum') / args.batch_size\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        #To use smart_regularization\n",
    "        # if args.use_smart_regularization:\n",
    "        #     smart_regularization(loss_value, args.smart_weight_regularization, embeddings, preds, model.last_layers_paraphrase)\n",
    "\n",
    "        objects_group.loss_sum += loss_value\n",
    "        \n",
    "        if args.projection == \"none\":\n",
    "            if args.use_amp: scaler.scale(loss).backward()\n",
    "            else: loss.backward()\n",
    "        return loss\n",
    "\n",
    "\n",
    "def process_similarity_batch(batch, objects_group: ObjectsGroup, args: dict):\n",
    "    '''This function processes a batch of similarity data. It takes as input a batch of data,\n",
    "    a group of objects (model, optimizer, scheduler, etc.), and the arguments. It returns the loss of the batch.'''\n",
    "    device = args.device\n",
    "    model, scaler = objects_group.model, objects_group.scaler\n",
    "\n",
    "    with autocast('cuda') if args.use_amp else nullcontext():\n",
    "        b_ids_1, b_mask_1, b_ids_2, b_mask_2, b_labels = (batch['token_ids_1'], batch['attention_mask_1'], batch['token_ids_2'], batch['attention_mask_2'], batch['labels'])\n",
    "        b_ids_1, b_mask_1, b_ids_2, b_mask_2, b_labels = b_ids_1.to(device), b_mask_1.to(device), b_ids_2.to(device), b_mask_2.to(device), b_labels.to(device)\n",
    "\n",
    "        embeddings = model.get_similarity_paraphrase_embeddings(b_ids_1, b_mask_1, b_ids_2, b_mask_2, task_id=2)\n",
    "        preds = model.last_layers_similarity(embeddings)\n",
    "        loss = F.mse_loss(preds.view(-1), b_labels.view(-1), reduction='sum') / args.batch_size\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        #To use smart_regularization\n",
    "        # if args.use_smart_regularization:\n",
    "        #     smart_regularization(loss_value, args.smart_weight_regularization, embeddings, preds, model.last_layers_similarity)\n",
    "\n",
    "        objects_group.loss_sum += loss_value\n",
    "        \n",
    "        if args.projection == \"none\":\n",
    "            if args.use_amp: scaler.scale(loss).backward()\n",
    "            else: loss.backward()\n",
    "        return loss\n",
    "\n",
    "def step_optimizer(objects_group: ObjectsGroup, args: dict, step: int, total_nb_batches = None):\n",
    "    \"\"\"Step the optimizer and update the scaler. Returns the loss\"\"\"\n",
    "    optimizer, scaler = objects_group.optimizer, objects_group.scaler\n",
    "    if args.use_amp:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    loss_value = objects_group.loss_sum\n",
    "    objects_group.loss_sum = 0\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(10004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args= {\n",
    "    'sst_file': '/home/interiit/hp3/uday/nlp_pr/train_data/final_sentiment.csv',\n",
    "    'para_file': '/home/interiit/hp3/uday/nlp_pr/train_data/final_paraphase.csv',\n",
    "    'sts_file':  '/home/interiit/hp3/uday/nlp_pr/train_data/final_similarity.csv',\n",
    "    'para_batch_size': 2,\n",
    "    'sst_batch_size': 2,\n",
    "    'sts_batch_size': 2,\n",
    "    'option': 'optimize',\n",
    "    'hidden_layers': 2,\n",
    "    'hidden_drp_prob': 0.2,\n",
    "    'lr': 1e-6,\n",
    "    'epochs': 2 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9810 train examples from /home/interiit/hp3/uday/nlp_pr/train_data/final_sentiment.csv\n",
      "Loaded 49401 train examples from /home/interiit/hp3/uday/nlp_pr/train_data/final_paraphase.csv\n",
      "Loaded 9840 train examples from /home/interiit/hp3/uday/nlp_pr/train_data/final_similarity.csv\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data(args['sst_file'],args['para_file'],args['sts_file'], split ='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'validation dataset for semantic textual similarity dataset'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_train_data = SentenceClassificationDataset(sst_train_data)\n",
    "sst_train_dataloader = DataLoader(sst_train_data, shuffle=True, batch_size= args['sst_batch_size'], collate_fn=sst_train_data.collate_fn)\n",
    "\"\"\"validation dataset for sentence classification dataset\"\"\"\n",
    "\n",
    "# Para: Paraphrase detection\n",
    "para_train_data = SentencePairDataset(para_train_data)\n",
    "para_train_dataloader = DataLoader(para_train_data, shuffle=True, batch_size= args['para_batch_size'],collate_fn=para_train_data.collate_fn)\n",
    "\"\"\"validation dataset for paraphase detection dataset\"\"\"\n",
    "\n",
    "# STS: Semantic textual similarity\n",
    "sts_train_data = SentencePairDataset(sts_train_data, isRegression=True)\n",
    "sts_train_dataloader = DataLoader(sts_train_data, shuffle=True, batch_size=args['sts_batch_size'],collate_fn=sts_train_data.collate_fn)\n",
    "\"\"\"validation dataset for semantic textual similarity dataset\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PCGrad used here'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'hidden_dropout_prob': args['hidden_drp_prob'],'num_labels': 5,'hidden_size': 768,'data_dir': '.','option': args['option'],'n_hidden_layers': args['hidden_layers']}\n",
    "config = SimpleNamespace(**config)\n",
    "model = Bert_MultiTask(config).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=args['lr'])\n",
    "scaler = None\n",
    "\n",
    "\"\"\"PCGrad used here\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package objects\n",
    "objects_group = ObjectsGroup(model, optimizer, scaler)\n",
    "args['device'] = device\n",
    "dataloaders = {'sst': sst_train_dataloader, 'para': para_train_dataloader, 'sts': sts_train_dataloader}\n",
    "scheduler = RoundRobinScheduler(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kernel Optimization:   0%|          | 0/4905 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Float and Half",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m b_ids, b_mask, b_labels \u001b[38;5;241m=\u001b[39m b_ids\u001b[38;5;241m.\u001b[39mto(device), b_mask\u001b[38;5;241m.\u001b[39mto(device), b_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(b_ids, b_mask, task_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_layers_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m logits \u001b[38;5;241m=\u001b[39m linear(logits)\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, b_labels)\n",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m, in \u001b[0;36mBert_MultiTask.last_layers_sentiment\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_sentiment) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     58\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_sentiment[i](x)\n\u001b[0;32m---> 59\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_sentiment\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_sentiment[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n",
      "File \u001b[0;32m~/miniconda3/envs/hp3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hp3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/hp3/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Float and Half"
     ]
    }
   ],
   "source": [
    "if args['option'] == \"optimize\":\n",
    "        # Run Kernel Optimization for SST\n",
    "    linear = nn.Linear(5,5, dtype=torch.float16)\n",
    "    W = np.eye(5).astype(np.float16)\n",
    "    B = np.array([0, 0, 0, 0, 0]).astype(np.float16)\n",
    "\n",
    "    # Init to W\n",
    "    linear.weight.data = torch.from_numpy(W).to(torch.float16).to(device)\n",
    "    linear.bias.data = torch.from_numpy(B).to(torch.float16).to(device)\n",
    "    linear.to(device)\n",
    "    optimizer = AdamW(linear.parameters(), lr=args['lr'])\n",
    "    # Compute accuracy on dev set\n",
    "    # model.last_linear_sentiment = linear\n",
    "    # dev_ac, _, _, _ = model_eval_sentiment(sst_dev_dataloader, model, device)\n",
    "    # print(Colors.BOLD + Colors.BLUE + \"Accuracy on dev set: \" + Colors.END + Colors.BLUE + str(dev_ac) + Colors.END)\n",
    "\n",
    "    # Print number of parameters for the optimizer\n",
    "    # print(Colors.BOLD + Colors.BLUE + \"Number of parameters for the optimizer: \" + Colors.END + Colors.BLUE + str(count_parameters(linear)) + Colors.END)\n",
    "    for epoch in range(args['epochs']):\n",
    "        \n",
    "        model.last_linear_sentiment = None\n",
    "        model.eval()\n",
    "        \n",
    "        for batch in tqdm(sst_train_dataloader, desc=\"Kernel Optimization\", smoothing=0):\n",
    "            b_ids, b_mask, b_labels = (batch['token_ids'], batch['attention_mask'], batch['labels'])\n",
    "            b_ids, b_mask, b_labels = b_ids.to(device), b_mask.to(device), b_labels.to(device)\n",
    "            embeddings = model.forward(b_ids, b_mask, task_id=0)\n",
    "            logits = model.last_layers_sentiment(embeddings)\n",
    "            logits = linear(logits)\n",
    "            loss = F.cross_entropy(logits, b_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Evaluate on dev set\n",
    "        # Actual evaluation\n",
    "        # model.last_linear_sentiment = linear\n",
    "        # dev_ac, _, _, _ = model_eval_sentiment(sst_dev_dataloader, model, device)\n",
    "        # print(Colors.BOLD + Colors.BLUE + \"Accuracy on dev set: \" + Colors.END + Colors.BLUE + str(dev_ac) + Colors.END)\n",
    "\n",
    "# Print weights of linear layer\n",
    "# print(Colors.BOLD + Colors.BLUE + \"Weights of linear layer: \" + Colors.END + Colors.BLUE + str(linear.weight.data) + Colors.END)\n",
    "# print(Colors.BOLD + Colors.BLUE + \"Bias of linear layer: \" + Colors.END + Colors.BLUE + str(linear.bias.data) + Colors.END)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
