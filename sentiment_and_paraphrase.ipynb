{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade sympy\n",
        "!pip install --force-reinstall sympy"
      ],
      "metadata": {
        "id": "uD4E12hTzthg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kaNiuVu-ef9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d13033-b8b6-4315-ae78-4e458cf994f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ympy (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ympy (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_scheduler\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from transformers.optimization import SchedulerType\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fix the random seed for reproducibility\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Multitask Model Wrapper\n",
        "class MultitaskModelWrapper:\n",
        "    def __init__(self, task_configs):\n",
        "        \"\"\"\n",
        "        Initialize multitask wrapper for different tasks.\n",
        "        Args:\n",
        "            task_configs (dict): Configuration for tasks including model and tokenizer.\n",
        "        \"\"\"\n",
        "        self.models = {}\n",
        "        self.tokenizers = {}\n",
        "        for task_name, config in task_configs.items():\n",
        "            self.models[task_name] = AutoModelForSequenceClassification.from_pretrained(\n",
        "                config[\"model_name\"],\n",
        "                num_labels=config[\"num_labels\"]\n",
        "            )\n",
        "            self.tokenizers[task_name] = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        for model in self.models.values():\n",
        "            model.to(self.device)\n",
        "\n",
        "    def train(self, dataloaders, epochs=3, lr=5e-5, weight_decay=0.01):\n",
        "        \"\"\"\n",
        "        Train models for all tasks.\n",
        "        Args:\n",
        "            dataloaders (dict): Dataloaders for tasks.\n",
        "            epochs (int): Number of epochs.\n",
        "            lr (float): Learning rate.\n",
        "            weight_decay (float): Weight decay for AdamW.\n",
        "        \"\"\"\n",
        "        optimizers = {\n",
        "            task: AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "            for task, model in self.models.items()\n",
        "        }\n",
        "        schedulers = {\n",
        "            task: get_scheduler(\n",
        "                name=SchedulerType.LINEAR,\n",
        "                optimizer=optimizers[task],\n",
        "                num_warmup_steps=0,\n",
        "                num_training_steps=epochs * len(dataloaders[task])\n",
        "            )\n",
        "            for task in self.models.keys()\n",
        "        }\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "            for task, model in self.models.items():\n",
        "                model.train()\n",
        "                total_loss = 0\n",
        "                for batch in tqdm(dataloaders[task], desc=f\"Training {task}\"):\n",
        "                    inputs = {k: v.to(self.device) for k, v in batch.items() if k != \"labels\"}\n",
        "                    labels = batch[\"labels\"].to(self.device)\n",
        "\n",
        "                    optimizers[task].zero_grad()\n",
        "                    outputs = model(**inputs, labels=labels)\n",
        "                    loss = outputs.loss\n",
        "                    loss.backward()\n",
        "                    optimizers[task].step()\n",
        "                    schedulers[task].step()\n",
        "                    total_loss += loss.item()\n",
        "                print(f\"{task} - Loss: {total_loss / len(dataloaders[task])}\")\n",
        "\n",
        "    def evaluate(self, dataloaders):\n",
        "        \"\"\"\n",
        "        Evaluate models on all tasks.\n",
        "        Args:\n",
        "            dataloaders (dict): Evaluation dataloaders.\n",
        "        Returns:\n",
        "            dict: Evaluation results for each task.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        for task, model in self.models.items():\n",
        "            model.eval()\n",
        "            correct, total = 0, 0\n",
        "            for batch in tqdm(dataloaders[task], desc=f\"Evaluating {task}\"):\n",
        "                inputs = {k: v.to(self.device) for k, v in batch.items() if k != \"labels\"}\n",
        "                labels = batch[\"labels\"].to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**inputs)\n",
        "                preds = torch.argmax(outputs.logits, dim=-1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "            results[task] = correct / total\n",
        "        return results\n",
        "\n"
      ],
      "metadata": {
        "id": "ytGzLx20h_lz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task Configurations\n",
        "task_configs = {\n",
        "    \"sentiment\": {\"model_name\": \"bert-base-uncased\", \"num_labels\": 5},\n",
        "    \"paraphrase\": {\"model_name\": \"roberta-base\", \"num_labels\": 2},\n",
        "    # \"similarity\": {\"model_name\": \"distilbert-base-uncased\", \"num_labels\": 1},\n",
        "}\n",
        "\n",
        "# Load Data (Example using Hugging Face's datasets library)\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_data(tokenizer, split=\"train\", max_samples=1000, task_name=None):\n",
        "\n",
        "    dataset = load_dataset(\"glue\", \"sst2\" if split == \"train\" else \"qqp\", split=split)\n",
        "    dataset = dataset.select(range(min(max_samples, len(dataset))))  # Reduce size\n",
        "\n",
        "    # Preprocessing function\n",
        "    def preprocess_function(examples):\n",
        "        tokenized = tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n",
        "        return tokenized\n",
        "\n",
        "    dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "    # Adjust label dtype based on task\n",
        "    if task_name == \"similarity\":  # For regression, use Float\n",
        "        dataset = dataset.map(lambda x: {\"labels\": float(x[\"label\"])})\n",
        "    else:  # For classification, use Long\n",
        "        dataset = dataset.rename_column(\"label\", \"labels\")  # Rename column\n",
        "        dataset = dataset.map(lambda x: {\"labels\": int(x[\"labels\"])})\n",
        "\n",
        "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "dataloaders = {}\n",
        "for task_name, config in task_configs.items():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
        "    data = load_data(tokenizer, split=\"train\", max_samples=500, task_name=task_name)\n",
        "    dataloaders[task_name] = DataLoader(data, batch_size=16, shuffle=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FtK3cqv_i57n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ANvtJuUumB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    seed_everything()\n",
        "    multitask_model = MultitaskModelWrapper(task_configs)\n",
        "    multitask_model.train(dataloaders, epochs=3)\n",
        "    results = multitask_model.evaluate(dataloaders)\n",
        "    print(\"Evaluation Results:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn0i-lGwr5zj",
        "outputId": "7a418093-5f77-4983-ee45-00a83f1103c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training sentiment: 100%|██████████| 32/32 [00:40<00:00,  1.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment - Loss: 0.883944833651185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training paraphrase: 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paraphrase - Loss: 0.6988671608269215\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training sentiment: 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment - Loss: 0.4566800412721932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training paraphrase: 100%|██████████| 32/32 [00:43<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paraphrase - Loss: 0.6731350738555193\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training sentiment: 100%|██████████| 32/32 [00:43<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment - Loss: 0.18095844145864248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training paraphrase: 100%|██████████| 32/32 [00:44<00:00,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paraphrase - Loss: 0.32627327437512577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating sentiment: 100%|██████████| 32/32 [00:15<00:00,  2.08it/s]\n",
            "Evaluating paraphrase: 100%|██████████| 32/32 [00:13<00:00,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'sentiment': 0.988, 'paraphrase': 0.93}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for task, result in results.items():\n",
        "  print(f\"Task: {task}, Result: {result}\")"
      ],
      "metadata": {
        "id": "-ip6weoJsAwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bccbc41a-404d-4d82-c308-522e272a2e17"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task: sentiment, Result: 0.988\n",
            "Task: paraphrase, Result: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3tof1Tgf0CzI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}